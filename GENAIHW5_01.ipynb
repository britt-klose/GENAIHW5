{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/britt-klose/GENAIHW5/blob/main/GENAIHW5_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-JQKIY7S-t9"
      },
      "source": [
        "GEN AI Assignment 5\n",
        "\n",
        "Brittany Klose\n",
        "\n",
        "11/14/24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AwfiRiyOBhfY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CTzvBb1QBihC"
      },
      "outputs": [],
      "source": [
        "#@title 0. Parameters\n",
        "\n",
        "VOCAB_SIZE = 1000\n",
        "MAX_LEN = 200\n",
        "EMBEDDING_DIM = 100\n",
        "N_UNITS = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "kpH91K2DBpO9",
        "outputId": "7c1e44a4-2447-4fe0-acc7-c998d9162c95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#@title 1. Load the data\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a6zde6uGBqvk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "urls = [\n",
        "     \"https://www.gutenberg.org/cache/epub/158/pg158.txt\",   # Emma\n",
        "     \"https://www.gutenberg.org/cache/epub/161/pg161.txt\",   # Sense & Sensibility\n",
        "     \"https://www.gutenberg.org/cache/epub/105/pg105.txt\",    # Persuasion\n",
        "     \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"   #Pride & Prejudice\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v_5DV5K-FNii"
      },
      "outputs": [],
      "source": [
        "#Clean individual texts\n",
        "#For loop to clean each text file by removing headers & footers\n",
        "# Goal is for each text to start at ch1 and end after the last chapter\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "  #text = re.sub(r'(?i)(Table of Contents|Contents)(.*?)(?=\\n)(?=\\n)', '', text, flags=re.DOTALL)\n",
        "  header = text.find(\"by Jane Austen\")\n",
        "  footer = text.find(\"*** END OF THE PROJECT GUTENBERG EBOOK \")\n",
        "  text = text[header:footer]\n",
        "  chapter1_header = re.search(r\"CHAPTER I \", text)\n",
        "  if chapter1_header:\n",
        "    text = text[chapter1_header.header():]\n",
        "\n",
        "  text = re.sub(r'(Preface).*?(VOLUME.\\.|volume\\.).*?(CHAPTER I.\\.|Chapter I\\.)', r'\\2', text, flags=re.DOTALL|re.IGNORECASE)\n",
        "  text = re.sub(r'(?i)(Table of Contents|Contents)(.*?)(?=\\n)(?=\\n)', '', text, flags=re.DOTALL)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jQbUIEyRB3Ab"
      },
      "outputs": [],
      "source": [
        "# Save all cleaned files to a single file\n",
        "def append_cleantxt(urls):\n",
        "\n",
        "  # Initialize an empty string to hold all text\n",
        "  all_text = \"\"\n",
        "\n",
        "  # Download each text file and append to all_text\n",
        "  for url in urls:\n",
        "    response = requests.get(url)\n",
        "    og_text = response.text\n",
        "\n",
        "    final_text=clean_text(og_text)\n",
        "\n",
        "    all_text += final_text + \"\\n\\n\"  # Separate texts by newlines\n",
        "\n",
        "  return all_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YZJpaIgtGV3b",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Call functions to clean and merge all texts into a single cleaned file\n",
        "\n",
        "all_cleaned_text = append_cleantxt(urls)\n",
        "\n",
        "# Save the cleaned text to a file\n",
        "with open('jane_works_cleaned.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(all_cleaned_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the words of text\n",
        "with open('jane_works_cleaned.txt', \"r\", encoding=\"utf-8\") as file:\n",
        "  file_content = file.read()\n",
        "  words = file_content.split()\n",
        "\n",
        "  n_words = len(words)\n",
        "print(f\"{n_words} words loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5LgDUhtt0iS",
        "outputId": "44c6d7f3-2168-4752-d3ed-21641b3e5c74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "359700 words loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preview_text = words[:200]\n",
        "print(f\"200 words of Jane: {preview_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKDWpQuTuE_6",
        "outputId": "df2d51da-1e4a-4fcb-c7d9-5e94a37216fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 words of Jane: ['by', 'Jane', 'Austen', 'VOLUME', 'I.', 'CHAPTER', 'I.', 'CHAPTER', 'II.', 'CHAPTER', 'III.', 'CHAPTER', 'IV.', 'CHAPTER', 'V.', 'CHAPTER', 'VI.', 'CHAPTER', 'VII.', 'CHAPTER', 'VIII.', 'CHAPTER', 'IX.', 'CHAPTER', 'X.', 'CHAPTER', 'XI.', 'CHAPTER', 'XII.', 'CHAPTER', 'XIII.', 'CHAPTER', 'XIV.', 'CHAPTER', 'XV.', 'CHAPTER', 'XVI.', 'CHAPTER', 'XVII.', 'CHAPTER', 'XVIII.', 'VOLUME', 'II.', 'CHAPTER', 'I.', 'CHAPTER', 'II.', 'CHAPTER', 'III.', 'CHAPTER', 'IV.', 'CHAPTER', 'V.', 'CHAPTER', 'VI.', 'CHAPTER', 'VII.', 'CHAPTER', 'VIII.', 'CHAPTER', 'IX.', 'CHAPTER', 'X.', 'CHAPTER', 'XI.', 'CHAPTER', 'XII.', 'CHAPTER', 'XIII.', 'CHAPTER', 'XIV.', 'CHAPTER', 'XV.', 'CHAPTER', 'XVI.', 'CHAPTER', 'XVII.', 'CHAPTER', 'XVIII.', 'VOLUME', 'III.', 'CHAPTER', 'I.', 'CHAPTER', 'II.', 'CHAPTER', 'III.', 'CHAPTER', 'IV.', 'CHAPTER', 'V.', 'CHAPTER', 'VI.', 'CHAPTER', 'VII.', 'CHAPTER', 'VIII.', 'CHAPTER', 'IX.', 'CHAPTER', 'X.', 'CHAPTER', 'XI.', 'CHAPTER', 'XII.', 'CHAPTER', 'XIII.', 'CHAPTER', 'XIV.', 'CHAPTER', 'XV.', 'CHAPTER', 'XVI.', 'CHAPTER', 'XVII.', 'CHAPTER', 'XVIII.', 'CHAPTER', 'XIX.', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse,', 'handsome,', 'clever,', 'and', 'rich,', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition,', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence;', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her.', 'She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate,', 'indulgent', 'father;', 'and', 'had,', 'in', 'consequence', 'of', 'her', 'sister’s', 'marriage,', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period.', 'Her', 'mother', 'had', 'died', 'too']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3vQmLoE6SFb1"
      },
      "outputs": [],
      "source": [
        "#@title 2. Tokenize the data\n",
        "# Pad the punctuation, to treat them as separate 'words'\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    return s\n",
        "with open(\"jane_works_cleaned.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = [pad_punctuation(line) for line in file]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "fgEGKGktZqZV",
        "outputId": "b5e54789-776f-4ba5-9e5a-e2a03cd6981a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rather too much her own way , and a disposition to think a little too\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "# Display an example of padded\n",
        "example_data = text_data[100]\n",
        "example_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8ip6Da6qSKfg"
      },
      "outputs": [],
      "source": [
        "# Convert to a Tensorflow Dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3J1FNM7ySV95"
      },
      "outputs": [],
      "source": [
        "# Create a vectorisation layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WgHwmldzSXZz"
      },
      "outputs": [],
      "source": [
        "# Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUshJp5mSYiN",
        "outputId": "b4588bc1-0ab5-4e68-c5f3-252cdef32a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: ,\n",
            "3: .\n",
            "4: the\n",
            "5: to\n",
            "6: and\n",
            "7: of\n",
            "8: a\n",
            "9: her\n",
            "10: was\n",
            "11: in\n",
            "12: i\n",
            "13: ;\n",
            "14: it\n",
            "15: she\n",
            "16: not\n",
            "17: be\n",
            "18: ”\n",
            "19: that\n"
          ]
        }
      ],
      "source": [
        "# Display some token:word mappings\n",
        "for i, word in enumerate(vocab[:20]):\n",
        "    print(f\"{i}: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VVCro6tJSaC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38658d32-5b45-464b-9c3f-05be6d7f6c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 38 189   1   1   1 192   1 192   1 192   1 192   1 192   1 192   1 192\n",
            "   1 192   1 192   1 192   1 192   1 192   1 192   1 192   1 192   1 192\n",
            "   1 192   1 192   1   1   1 192   1 192   1 192   1 192   1 192   1 192\n",
            "   1 192   1 192   1 192   1 192   1 192   1 192   1 192   1 192   1 192\n",
            "   1 192   1 192   1 192   1   1   1 192   1 192   1 192   1 192   1 192\n",
            "   1 192   1 192   1 192   1 192   1 192   1 192   1 192   1 192   1 192\n",
            "   1 192   1 192   1 192   1 192   1 192   1   1  12 192  12  79   1   1\n",
            "   1   6   1  25   8 563 180   6 179   1 154   5   1  99   7   4 315   1\n",
            "   7   1   6  20 653   1   1 313  11   4 240  25  31  87   5 710  60   1\n",
            "   1  15  10   4   1   7   4 124 983   7   8 109   1   1   1   6   1  11\n",
            " 562   7   9 616   1  39   1   7  26 146  49   8  31 517   1   9 166  20\n",
            "   1 101 147]\n"
          ]
        }
      ],
      "source": [
        "# Display the same example converted to ints\n",
        "example_tokenised = vectorize_layer(all_cleaned_text)\n",
        "print(example_tokenised.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FQvLjsRuSbZR"
      },
      "outputs": [],
      "source": [
        "#@title 3. Create the training set of recipes and the same text shifted by one word\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aA3CrCcpSdAP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "9ba0bcea-8e24-4348-b66c-1d795f58c2b7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │         \u001b[38;5;34m100,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)          │         \u001b[38;5;34m129,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">100,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">129,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m346,248\u001b[0m (1.32 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">346,248</span> (1.32 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m346,248\u001b[0m (1.32 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">346,248</span> (1.32 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title 4. Build the LSTM\n",
        "\n",
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
        "# Last LSTM Layer if needed\n",
        "#x = layers.LSTM(N_UNITS)(x)      # return_sequences=False by default\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm = models.Model(inputs, outputs)\n",
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CBDdcf8SSe-L"
      },
      "outputs": [],
      "source": [
        "#@title 5. Train the LSTM\n",
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm.compile(\"adam\", loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CH8JbcMcShOs"
      },
      "outputs": [],
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index for index, word in enumerate(index_to_word)\n",
        "        }  # <1>\n",
        "\n",
        "    def sample_from(self, probs, temperature):  # <2>\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]  # <3>\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:  # <4>\n",
        "            x = np.array([start_tokens])\n",
        "            y = self.model.predict(x, verbose=0)  # <5>\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)  # <6>\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            start_tokens.append(sample_token)  # <7>\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"Jane Quote:\", max_tokens=100, temperature=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XU7KnvIoSiUv"
      },
      "outputs": [],
      "source": [
        "# Tokenize starting prompt\n",
        "\n",
        "text_generator = TextGenerator(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9EAxZ0rJSkfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4410af00-814c-4015-d113-96e141e88d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - loss: 0.6135\n",
            "generated text:\n",
            "Jane Quote: all call might [UNK] [UNK] there were . \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m553s\u001b[0m 469ms/step - loss: 0.6133\n",
            "Epoch 2/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.2503\n",
            "generated text:\n",
            "Jane Quote: of . my few [UNK] , ” said the [UNK] , [UNK] under least \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 468ms/step - loss: 0.2503\n",
            "Epoch 3/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - loss: 0.2231\n",
            "generated text:\n",
            "Jane Quote: , to be that she was wanted in expected [UNK] , . all [UNK] has \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 469ms/step - loss: 0.2231\n",
            "Epoch 4/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.2147\n",
            "generated text:\n",
            "Jane Quote: , what his event were at her [UNK] to her \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 468ms/step - loss: 0.2147\n",
            "Epoch 5/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.2068\n",
            "generated text:\n",
            "Jane Quote: and [UNK] miss woodhouse , ” replied miss [UNK] fanny \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 468ms/step - loss: 0.2068\n",
            "Epoch 6/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.2037\n",
            "generated text:\n",
            "Jane Quote: there , all only emma may would prevent his own \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 468ms/step - loss: 0.2037\n",
            "Epoch 7/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 0.1996\n",
            "generated text:\n",
            "Jane Quote: were [UNK] , they [UNK] , for willoughby \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 465ms/step - loss: 0.1996\n",
            "Epoch 8/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1987\n",
            "generated text:\n",
            "Jane Quote: of [UNK] which might [UNK] . \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 466ms/step - loss: 0.1987\n",
            "Epoch 9/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1958\n",
            "generated text:\n",
            "Jane Quote: of mr elliot , that one [UNK] [UNK] in many \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 465ms/step - loss: 0.1958\n",
            "Epoch 10/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 0.1937\n",
            "generated text:\n",
            "Jane Quote: at all . \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 464ms/step - loss: 0.1937\n",
            "Epoch 11/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 0.1924\n",
            "generated text:\n",
            "Jane Quote: of the [UNK] , and lady [UNK] a friend \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 464ms/step - loss: 0.1924\n",
            "Epoch 12/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - loss: 0.1882\n",
            "generated text:\n",
            "Jane Quote: than his hands . \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 468ms/step - loss: 0.1882\n",
            "Epoch 13/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 0.1897\n",
            "generated text:\n",
            "Jane Quote: turn in their [UNK] [UNK] . \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 465ms/step - loss: 0.1897\n",
            "Epoch 14/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1871\n",
            "generated text:\n",
            "Jane Quote: a letter he never had suffered or [UNK] in almost \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 466ms/step - loss: 0.1871\n",
            "Epoch 15/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.1845\n",
            "generated text:\n",
            "Jane Quote: with four [UNK] [UNK] engagement , and [UNK] captain \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1845\n",
            "Epoch 16/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1853\n",
            "generated text:\n",
            "Jane Quote: ! good is letter , i love , you are very obliging . but upon \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 467ms/step - loss: 0.1853\n",
            "Epoch 17/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - loss: 0.1831\n",
            "generated text:\n",
            "Jane Quote: said in real [UNK] , he was [UNK] to get and miss \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 468ms/step - loss: 0.1831\n",
            "Epoch 18/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1834\n",
            "generated text:\n",
            "Jane Quote: to [UNK] [UNK] [UNK] impossible . \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 465ms/step - loss: 0.1834\n",
            "Epoch 19/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1841\n",
            "generated text:\n",
            "Jane Quote: such [UNK] and [UNK] as her eyes , \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 466ms/step - loss: 0.1841\n",
            "Epoch 20/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 0.1795\n",
            "generated text:\n",
            "Jane Quote: of [UNK] that , so , as an [UNK] [UNK] by \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 464ms/step - loss: 0.1795\n",
            "Epoch 21/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1806\n",
            "generated text:\n",
            "Jane Quote: of the [UNK] could make the more [UNK] \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1806\n",
            "Epoch 22/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1778\n",
            "generated text:\n",
            "Jane Quote: , elinor , beyond her own way , and [UNK] to each \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 466ms/step - loss: 0.1778\n",
            "Epoch 23/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1776\n",
            "generated text:\n",
            "Jane Quote: to [UNK] her [UNK] , [UNK] her as they \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 466ms/step - loss: 0.1776\n",
            "Epoch 24/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1764\n",
            "generated text:\n",
            "Jane Quote: in its [UNK] [UNK] ? ” \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 466ms/step - loss: 0.1764\n",
            "Epoch 25/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1771\n",
            "generated text:\n",
            "Jane Quote: . he got only , however , that his best praise would \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1771\n",
            "Epoch 26/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1763\n",
            "generated text:\n",
            "Jane Quote: of [UNK] . \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 465ms/step - loss: 0.1763\n",
            "Epoch 27/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - loss: 0.1743\n",
            "generated text:\n",
            "Jane Quote: ; and all that she had [UNK] herself looking at her \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 468ms/step - loss: 0.1743\n",
            "Epoch 28/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 0.1743\n",
            "generated text:\n",
            "Jane Quote: . short i could [UNK] [UNK] [UNK] , and i \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 465ms/step - loss: 0.1743\n",
            "Epoch 29/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.1734\n",
            "generated text:\n",
            "Jane Quote: , by her father . [UNK] letter was not open , and \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1734\n",
            "Epoch 30/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1733\n",
            "generated text:\n",
            "Jane Quote: ! ” \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 466ms/step - loss: 0.1733\n",
            "Epoch 31/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 0.1725\n",
            "generated text:\n",
            "Jane Quote: ! so ! ’ appear to think . good being [UNK] than i can ; some \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 465ms/step - loss: 0.1725\n",
            "Epoch 32/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1708\n",
            "generated text:\n",
            "Jane Quote: company , and walk her father’s [UNK] in another \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 466ms/step - loss: 0.1708\n",
            "Epoch 33/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1698\n",
            "generated text:\n",
            "Jane Quote: , could [UNK] [UNK] her from that end reached the \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1698\n",
            "Epoch 34/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1705\n",
            "generated text:\n",
            "Jane Quote: ; the [UNK] , the many [UNK] of anne was standing ; and was \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1705\n",
            "Epoch 35/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1682\n",
            "generated text:\n",
            "Jane Quote: from her mother , and only for a moment of her mother and \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 466ms/step - loss: 0.1682\n",
            "Epoch 36/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1684\n",
            "generated text:\n",
            "Jane Quote: ; [UNK] distress against her . and had he heard , \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1684\n",
            "Epoch 37/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1675\n",
            "generated text:\n",
            "Jane Quote: it [UNK] to [UNK] half a [UNK] pleasure , and the [UNK] , \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 465ms/step - loss: 0.1675\n",
            "Epoch 38/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1687\n",
            "generated text:\n",
            "Jane Quote: and [UNK] as he had so [UNK] ease to \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 466ms/step - loss: 0.1687\n",
            "Epoch 39/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 0.1667\n",
            "generated text:\n",
            "Jane Quote: ; and there had nothing to [UNK] him , that the \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 465ms/step - loss: 0.1667\n",
            "Epoch 40/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1661\n",
            "generated text:\n",
            "Jane Quote: . they were [UNK] a [UNK] old woman , but a \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1661\n",
            "Epoch 41/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1665\n",
            "generated text:\n",
            "Jane Quote: for the [UNK] of the [UNK] , with all the \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 466ms/step - loss: 0.1665\n",
            "Epoch 42/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1662\n",
            "generated text:\n",
            "Jane Quote: could [UNK] for him to himself , even there was \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1662\n",
            "Epoch 43/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - loss: 0.1632\n",
            "generated text:\n",
            "Jane Quote: ! [UNK] , come ! there is no [UNK] before him . ” \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 466ms/step - loss: 0.1632\n",
            "Epoch 44/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1634\n",
            "generated text:\n",
            "Jane Quote: it in a change of family had been given , but , she came , \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1634\n",
            "Epoch 45/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1644\n",
            "generated text:\n",
            "Jane Quote: could even [UNK] to say . —she wished to be \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 466ms/step - loss: 0.1644\n",
            "Epoch 46/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1625\n",
            "generated text:\n",
            "Jane Quote: a little [UNK] she soon . in wonder that she \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1625\n",
            "Epoch 47/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 0.1621\n",
            "generated text:\n",
            "Jane Quote: ? ” cried emma . “i heard it i am to guess a _ one _ ? ” \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1621\n",
            "Epoch 48/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.1619\n",
            "generated text:\n",
            "Jane Quote: of [UNK] anxiety of the [UNK] , and that a \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 468ms/step - loss: 0.1619\n",
            "Epoch 49/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.1600\n",
            "generated text:\n",
            "Jane Quote: too well upon her , and mr [UNK] was not put upon a [UNK] \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 468ms/step - loss: 0.1600\n",
            "Epoch 50/50\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.1618\n",
            "generated text:\n",
            "Jane Quote: ! ” said sir john . [UNK] five couple , who have ever too \n",
            "\n",
            "\u001b[1m1173/1173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 467ms/step - loss: 0.1618\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b4c670660e0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "lstm.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sw5nhOPSSlji"
      },
      "outputs": [],
      "source": [
        "#@title Generate text using the LSTM\n",
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "        print(\"--------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "srmMhBSsSnQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13881db0-2528-416d-a792-64ccf493f342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "You pierce my soul. I am half agony, half hope. Tell me not that I am too\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"You pierce my soul. I am half agony, half hope. Tell me not that I am too\", max_tokens=10, temperature=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "L3h5g5tDSohn"
      },
      "outputs": [],
      "source": [
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zQxH8PYiSp3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abed8fea-01d7-4363-f1a7-d8a8302cc4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "You pierce my soul. I am half agony, half hope. Tell me not that I am too\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"You pierce my soul. I am half agony, half hope. Tell me not that I am too\", max_tokens=10, temperature=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_v7PEJ3TSrlN"
      },
      "outputs": [],
      "source": [
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wo1bvp2QStJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4228be-d788-41a3-cff2-17ceed19cfd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "To wish was to hope, and to hope was to expect\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"To wish was to hope, and to hope was to expect\", max_tokens=7, temperature=1.0\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AA11-3BVSvSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28cfb572-b649-4ceb-ab28-64206dbc509d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "To wish was to hope, and to hope was to expect to [UNK] \n",
            "\n",
            "\n",
            "PROMPT: To wish was to hope, and to hope was to expect\n",
            "to:   \t94.07%\n",
            "the:   \t4.86%\n",
            ".:   \t0.4%\n",
            "that:   \t0.19%\n",
            "herself:   \t0.16%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To wish was to hope, and to hope was to expect to\n",
            "[UNK]:   \t99.99%\n",
            "any:   \t0.01%\n",
            "marry:   \t0.0%\n",
            "be:   \t0.0%\n",
            "a:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To wish was to hope, and to hope was to expect to [UNK]\n",
            ":   \t99.8%\n",
            ",:   \t0.11%\n",
            ";:   \t0.04%\n",
            ".:   \t0.04%\n",
            "in:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"To wish was to hope, and to hope was to expect\", max_tokens=50, temperature=0.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_vFsu71SxmG"
      },
      "source": [
        "Discussion & Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkE-CrHcS3jV"
      },
      "source": [
        "3. Experiment with Model Complexity\n",
        "\n",
        "* Increase the number of LSTM layers\n",
        "* Adjust the number of units in each LSTM layer\n",
        "\n",
        "Results:\n",
        "\n",
        "Adding additional LSTM layers helped my program though more in regards to adding more words than necessarily adding clarity to the result. I didn’t add more than 3 layers as I was afraid it would lead to overfitting. I then tried to simply adjust the number of units in each LSTM layer. I did two layers, one with 64 units and one with 128 units. This produced slightly better output but not by much. Additionally, when I first added in  extra layers I experienced errors around 18 epochs that would prevent the code from continuing its execution. I had to update some of my definitions to include more scripts to match the complexity of the model. In this last run I went back to my original amount of layers again and just increased the epochs from 25 to 50 to see if it would help as opposed to increasing the layers. Unfortunately by then I had reached the limit for using a better run time so the model took an especially long time to load, and in the end the results seemed just as bad at the first time with 25 epochs, though this may have been because I lowered the vocab size back down to 10,000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skQmDXoaS4Y0"
      },
      "source": [
        "4. Temperature and Prompt Variations\n",
        "\n",
        "* Test various seed prompts to generate text.\n",
        "* Experiment with different temperature settings\n",
        "* Analyze the generated outputs for each prompt and temperature combination.\n",
        "\n",
        "Results:\n",
        "\n",
        " For this program I tested using the same prompt 4 times, using 4 different prompts, and then using the same 2 prompts twice as seen in my last test run to see how it would affect performance. Using 4 different prompts introduced more diversity and flexibility in results though they did not do as well keeping the consistency of Jane Austen’s iconic dramatic flare and longing and agonizing tone. On the contrary when I did the same prompt 4 times I had much less diversity but the results were more consistent in tone. When I used 2 prompts 2 times it was about a mix between the previous steps in diversity and consistency. In this last run I made quotes slightly longer to see if it would produce better results however I forgot to adjust the max_tokens to be larger so all but the last prompt returned nothing. And as mentioned earlier this run took a cruel amount of time to process so I did not rerun it, but if I did it again I would obviously update the tokens for better results.\n",
        "\n",
        "Regarding temperature effects, lower temperatures provided very little output and the creativity was very low, usually offering a random word or two that was frankly so mundane and nowhere near poetic or dramatic enough to be in a Jane Austen novel. I hoped with increasing the temperature the program would add more words and get more detailed, which it did, but adding more detail didn’t quite finish the thought/prompt the way I intended. Instead it made some parts too long and would miss the theme. To elaborate, all the prompts I chose related to agonizing over love in some way so my goal was for the model to predict words related to love and tragedy but the model did not often catch it, though it seemed closer when the temperature was lower. Though maybe it just seemed that way because less words were included so sentences didn’t seem too long and unfocused. Unfortunately this can’t be seen well in this last run but looking at the last prompt with temperature 0.2 the results only provided an additional word or two yet the incompleteness of the sentence leaving the thought not quite finished seemed to relate more to Austen’s tortured love language of longing but not quite reaching it than a bunch of extra detail in the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZagi8R0S6ue"
      },
      "source": [
        "5. Evaluation of Generated Text\n",
        "\n",
        "Results:\n",
        "\n",
        "The quality of the generated text wasn’t as great as one would hope for but it wasn’t any worse than I was expecting going into this. For the complexity of the model and the amount of training put into it I thought the results were not bad. Relevance and stylistic accuracy overall was somewhat low, but I wasn’t surprised by this. When I increased complexity and vocab size it was definitely better and I could see it improving more so if I added in more prompts for testing. Additionally, coherence was pretty decent especially in the lower temperatures as only a few words were added. All in all, this was a really cool and fun assignment and I look forward to doing more testing with this kind of model going forward."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdev54Y63tv5uZ4N/08ReZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}